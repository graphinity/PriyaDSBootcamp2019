{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data leakage, imbalanced datasets,cross validation, pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922103</td>\n",
       "      <td>0.057794</td>\n",
       "      <td>0.526038</td>\n",
       "      <td>0.148553</td>\n",
       "      <td>0.568629</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327477</td>\n",
       "      <td>0.820193</td>\n",
       "      <td>0.959226</td>\n",
       "      <td>0.419158</td>\n",
       "      <td>0.289422</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015102</td>\n",
       "      <td>0.064755</td>\n",
       "      <td>0.877044</td>\n",
       "      <td>0.400378</td>\n",
       "      <td>0.466417</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.368077</td>\n",
       "      <td>0.896085</td>\n",
       "      <td>0.572603</td>\n",
       "      <td>0.500439</td>\n",
       "      <td>0.736777</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.584785</td>\n",
       "      <td>0.882418</td>\n",
       "      <td>0.336971</td>\n",
       "      <td>0.228047</td>\n",
       "      <td>0.982025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4  customer_id  label\n",
       "0  0.922103  0.057794  0.526038  0.148553  0.568629            1      1\n",
       "1  0.327477  0.820193  0.959226  0.419158  0.289422            2      1\n",
       "2  0.015102  0.064755  0.877044  0.400378  0.466417            3      1\n",
       "3  0.368077  0.896085  0.572603  0.500439  0.736777            4      1\n",
       "4  0.584785  0.882418  0.336971  0.228047  0.982025            5      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame(np.random.rand(100,5))\n",
    "\n",
    "df['customer_id']= range(1,101)\n",
    "\n",
    "df['label'] = [1]*25 + [0]*75\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df.drop(columns='label')\n",
    "y=df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        24\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        33\n",
      "   macro avg       1.00      1.00      1.00        33\n",
      "weighted avg       1.00      1.00      1.00        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEvCAYAAAC+HYFkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEONJREFUeJzt3X+s3XV9x/HnixZkcw639WpcWyiZNa5O5McNanQOJsuK09Ys/qAG3RyxySKC0bnB3NCxmG0aZ1zC3BpFp04QyTYb18kcom4qpMUfQOm6dSjjpjKqIoioWHzvj3Mqt6envefcnt7v7cfnI2m83+/5eO/7ID758rnfc06qCklSW47pegBJ0uQZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYt7eoHL1u2rFatWtXVj5eko9LNN9/89aqammtdZ3FftWoV27Zt6+rHS9JRKcmdo6xzW0aSGmTcJalBxl2SGmTcJalBxl2SGjRn3JNcmeSeJLcd5PEk+asku5LckuT0yY8pSRrHKFfu7wPWHuLxc4HV/T8bgXcd/liSpMMxZ9yr6jPANw+xZD3w/uq5EXhskidMakBJ0vgmsee+HLhr1vFM/5wkqSOTeIVqhpwb+qnbSTbS27rhxBNPnMCPlqTJePtLn7+gP+/1H/7YEf3+k7hynwFWzjpeAewetrCqNlXVdFVNT03N+dYIkqR5mkTcNwOv6N818wzgvqr62gS+ryRpnubclklyFXAWsCzJDPAm4FiAqvobYAvwPGAX8CDwyiM1rCRpNHPGvao2zPF4Aa+e2ESSpMPmK1QlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaNFLck6xNsjPJriSXDHn8xCQ3JPlikluSPG/yo0qSRjVn3JMsAa4AzgXWABuSrBlY9kfANVV1GnAe8NeTHlSSNLpRrtzPBHZV1R1V9RBwNbB+YE0BP93/+gRg9+RGlCSNa+kIa5YDd806ngGePrDmzcC/JnkN8GjgnIlMJ0mal1Gu3DPkXA0cbwDeV1UrgOcBH0hywPdOsjHJtiTb9uzZM/60kqSRjBL3GWDlrOMVHLjtcgFwDUBVfR44Hlg2+I2qalNVTVfV9NTU1PwmliTNaZS4bwVWJzk5yXH0fmG6eWDN/wLPBUjyi/Ti7qW5JHVkzrhX1V7gQuA6YAe9u2K2J7k8ybr+stcDr0ryZeAq4LeranDrRpK0QEb5hSpVtQXYMnDusllf3w48a7KjSZLmy1eoSlKDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDRop7krVJdibZleSSg6x5SZLbk2xP8qHJjilJGsfSuRYkWQJcAfwaMANsTbK5qm6ftWY1cCnwrKq6N8njjtTAkqS5jXLlfiawq6ruqKqHgKuB9QNrXgVcUVX3AlTVPZMdU5I0jlHivhy4a9bxTP/cbE8CnpTks0luTLJ22DdKsjHJtiTb9uzZM7+JJUlzGiXuGXKuBo6XAquBs4ANwLuTPPaA/1LVpqqarqrpqampcWeVJI1olLjPACtnHa8Adg9Z89Gq+kFVfQXYSS/2kqQOjBL3rcDqJCcnOQ44D9g8sOafgLMBkiyjt01zxyQHlSSNbs64V9Ve4ELgOmAHcE1VbU9yeZJ1/WXXAd9IcjtwA/CGqvrGkRpaknRoc94KCVBVW4AtA+cum/V1Aa/r/5EkdcxXqEpSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDVopLgnWZtkZ5JdSS45xLoXJakk05MbUZI0rjnjnmQJcAVwLrAG2JBkzZB1jwEuAm6a9JCSpPGMcuV+JrCrqu6oqoeAq4H1Q9b9KfBW4HsTnE+SNA+jxH05cNes45n+uR9Jchqwsqo+NsHZJEnzNErcM+Rc/ejB5BjgHcDr5/xGycYk25Js27Nnz+hTSpLGMkrcZ4CVs45XALtnHT8G+CXgU0m+CjwD2Dzsl6pVtamqpqtqempqav5TS5IOaZS4bwVWJzk5yXHAecDmfQ9W1X1VtayqVlXVKuBGYF1VbTsiE0uS5jRn3KtqL3AhcB2wA7imqrYnuTzJuiM9oCRpfEtHWVRVW4AtA+cuO8jasw5/LEnS4fAVqpLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUoJHinmRtkp1JdiW5ZMjjr0tye5Jbklyf5KTJjypJGtWccU+yBLgCOBdYA2xIsmZg2ReB6ao6BbgWeOukB5UkjW6UK/czgV1VdUdVPQRcDayfvaCqbqiqB/uHNwIrJjumJGkco8R9OXDXrOOZ/rmDuQD4l2EPJNmYZFuSbXv27Bl9SknSWEaJe4acq6ELk/OBaeBtwx6vqk1VNV1V01NTU6NPKUkay9IR1swAK2cdrwB2Dy5Kcg7wRuBXqur7kxlPkjQfo1y5bwVWJzk5yXHAecDm2QuSnAb8LbCuqu6Z/JiSpHHMGfeq2gtcCFwH7ACuqartSS5Psq6/7G3ATwEfSfKlJJsP8u0kSQtglG0ZqmoLsGXg3GWzvj5nwnNJkg6Dr1CVpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lq0EhxT7I2yc4ku5JcMuTxRyX5cP/xm5KsmvSgkqTRzRn3JEuAK4BzgTXAhiRrBpZdANxbVU8E3gH8xaQHlSSNbpQr9zOBXVV1R1U9BFwNrB9Ysx74u/7X1wLPTZLJjSlJGscocV8O3DXreKZ/buiaqtoL3Af83CQGlCSNb+kIa4Zdgdc81pBkI7Cxf/hAkp0j/PxJWQZ8fQF/3kLz+R29Wn5u4PMb6veumffmxkmjLBol7jPAylnHK4DdB1kzk2QpcALwzcFvVFWbgE2jDDZpSbZV1XQXP3sh+PyOXi0/N/D5dWWUbZmtwOokJyc5DjgP2DywZjPwW/2vXwR8sqoOuHKXJC2MOa/cq2pvkguB64AlwJVVtT3J5cC2qtoMvAf4QJJd9K7YzzuSQ0uSDm2UbRmqaguwZeDcZbO+/h7w4smONnGdbActIJ/f0avl5wY+v07E3RNJao9vPyBJDTLuktSgkfbcj0ZJnkzvlbPL6d1zvxvYXFU7Oh1MP/aSnAlUVW3tv5XHWuA/+7/bak6S91fVK7qe48dNk3vuSf4A2EDvrRJm+qdX0LuL5+qq+vOuZtPc+v9gXg7cVFUPzDq/tqo+3t1khy/Jm+i9T9NS4BPA04FPAecA11XVW7qb7vAlGbxNOsDZwCcBqmrdgg81QUlOP9TjVfWFhZplLq3G/b+Ap1TVDwbOHwdsr6rV3Ux25CV5ZVW9t+s55ivJRcCrgR3AqcDFVfXR/mNfqKpD/p9rsUtyK73n9SjgbmBFVd2f5Cfo/cPslE4HPExJvgDcDryb3r8xB7iK/u3RVfXp7qY7fElu6H95PDANfJneczyF3v9+z+5qtkGt7rn/EPj5Ieef0H+sZX/S9QCH6VXAGVX1QuAs4I+TXNx/rIU3o9tbVQ9X1YPA/1TV/QBV9V3a+HtzGrgZeCNwX1V9CvhuVX36aA87QFWdXVVnA3cCp1fVdFWdAZwG7Op2uv21uuf+WuD6JP/NI296diLwRODCzqaakCS3HOwh4PELOcsRsGTfVkxVfTXJWcC1SU6ijbg/lOQn+3E/Y9/JJCfQQNyr6ofAO5J8pP+f/0ebnXlyVd2676CqbktyapcDDWrxLzpV9fEkT6L3dsXL6UVhBthaVQ93OtxkPB74deDegfMBPrfw40zU3UlOraovAVTVA0meD1wJPLXb0SbiOVX1ffhRCPc5lkfewuOoV1UzwIuT/AZwf9fzHAE7krwb+CC97afz6W0lLhpN7rm3Lsl7gPdW1X8MeexDVfWyDsaaiCQr6G1d3D3ksWdV1Wc7GEvaT5Ljgd8FntM/9RngXf1X6y8Kxl2SGtTktowkHQlJrqmql/Tvejrgyngx3e3klbskjSjJE6rqa/1f8B+gqu5c6JkOxrhL0oQl+XxVPbPLGVq9z12SunR81wMYd0mavM63RIy7JDXIuEvS5HX+amrjLkljSLIkyb/NsezlCzLMIRh3SRpD/y1MHuy/H9DB1ty2gCMN5YuYJGl83wNuTfIJ4Dv7TlbVRd2NtD/jLknj++f+n0XLFzFJ0jz0P2DlxKra2fUsw7jnLkljSvIC4EvAx/vHpw75iMFOGXdJGt+b6X1exLcA+p8/cHKXAw0y7pI0vr1Vdd/AuUW1x+0vVCVpfLcleRmwJMlq4CIW2aegeeUuSeN7DfAU4PvAVfQ+SvC1nU40wLtlJKlBbstI0piSTAN/CKxiVkf9JCZJOool2Qm8AbgV+OG+84vpk5i8cpek8e2pqkV1X/sgr9wlaUxJngtsAK6n90tVAKrqHzobaoBX7pI0vlcCTwaO5ZFtmQKMuyQdxZ5WVU/teohD8T53SRrfjUnWdD3EobjnLkljSrID+AXgK/T23AOUt0JK0lEsyUnDzi+mWyGNuyTNQ5KnAb/cP/z3qvpyl/MMcs9dksaU5GLg74HH9f98MMlrup1qf165S9KYktwCPLOqvtM/fjTw+cW05+6VuySNL8DDs44f7p9bNLzPXZLG917gpiT/2D9+IXBlh/McwG0ZSZqHJKcDz6Z3xf6ZqvpixyPtx7hL0piSfKCqXj7XuS655y5J43vK7IMkS4AzOpplKOMuSSNKcmmSbwOnJLm//+fbwD3ARzsebz9uy0jSmJL8WVVd2vUch+KVuySN72P9e9tJcn6SvzzYWxJ0xbhL0vjeBTzYfwuC3wfuBN7f7Uj7M+6SNL691dvTXg+8s6reCTym45n244uYJGl8305yKXA+8Jz+3TLHdjzTfrxyl6TxvZTe+7hfUFV3A8uBt3U70v68W0aSGuS2jCSNqX9v+74r4+Pobck8UFUndDfV/oy7JI2pqvb75WmSFwJndjTOUG7LSNIEJLmxqp7R9Rz7eOUuSWNK8puzDo8Bpnlkm2ZRMO6SNL4XzPp6L/BVYF03owxn3CVpfMcAF1fVtwCS/AzwduB3Op1qFu9zl6TxnbIv7ABVdS9wWofzHMC4S9L4julfrQOQ5GdZZDshi2oYSTpKvB34XJJr6f0i9SXAW7odaX/eCilJ85BkDfCr9D5D9fqqur3jkfZj3CWpQe65S1KDjLskNci4S1KDjLskNci4S1KD/h//2VVBJQ0NGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(index=X.columns, data=model.feature_importances_).plot('bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922103</td>\n",
       "      <td>0.057794</td>\n",
       "      <td>0.526038</td>\n",
       "      <td>0.148553</td>\n",
       "      <td>0.568629</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327477</td>\n",
       "      <td>0.820193</td>\n",
       "      <td>0.959226</td>\n",
       "      <td>0.419158</td>\n",
       "      <td>0.289422</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015102</td>\n",
       "      <td>0.064755</td>\n",
       "      <td>0.877044</td>\n",
       "      <td>0.400378</td>\n",
       "      <td>0.466417</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.368077</td>\n",
       "      <td>0.896085</td>\n",
       "      <td>0.572603</td>\n",
       "      <td>0.500439</td>\n",
       "      <td>0.736777</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.584785</td>\n",
       "      <td>0.882418</td>\n",
       "      <td>0.336971</td>\n",
       "      <td>0.228047</td>\n",
       "      <td>0.982025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.351409</td>\n",
       "      <td>0.713414</td>\n",
       "      <td>0.452940</td>\n",
       "      <td>0.687918</td>\n",
       "      <td>0.415270</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.221729</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.704187</td>\n",
       "      <td>0.787040</td>\n",
       "      <td>0.811190</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.134116</td>\n",
       "      <td>0.788072</td>\n",
       "      <td>0.501306</td>\n",
       "      <td>0.686186</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.503838</td>\n",
       "      <td>0.373508</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.597098</td>\n",
       "      <td>0.458992</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.932887</td>\n",
       "      <td>0.331860</td>\n",
       "      <td>0.702883</td>\n",
       "      <td>0.884695</td>\n",
       "      <td>0.383202</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.462645</td>\n",
       "      <td>0.750930</td>\n",
       "      <td>0.197640</td>\n",
       "      <td>0.822586</td>\n",
       "      <td>0.200114</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.137557</td>\n",
       "      <td>0.050172</td>\n",
       "      <td>0.658260</td>\n",
       "      <td>0.209058</td>\n",
       "      <td>0.843895</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.687471</td>\n",
       "      <td>0.395025</td>\n",
       "      <td>0.495986</td>\n",
       "      <td>0.647649</td>\n",
       "      <td>0.432749</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.695643</td>\n",
       "      <td>0.408796</td>\n",
       "      <td>0.861199</td>\n",
       "      <td>0.377008</td>\n",
       "      <td>0.634284</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.937113</td>\n",
       "      <td>0.076653</td>\n",
       "      <td>0.545357</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.804219</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.068962</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.623262</td>\n",
       "      <td>0.940664</td>\n",
       "      <td>0.480909</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.694689</td>\n",
       "      <td>0.415122</td>\n",
       "      <td>0.365560</td>\n",
       "      <td>0.467489</td>\n",
       "      <td>0.833134</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.657428</td>\n",
       "      <td>0.132193</td>\n",
       "      <td>0.268241</td>\n",
       "      <td>0.296970</td>\n",
       "      <td>0.187559</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.381308</td>\n",
       "      <td>0.174895</td>\n",
       "      <td>0.600854</td>\n",
       "      <td>0.929272</td>\n",
       "      <td>0.901171</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.771835</td>\n",
       "      <td>0.304107</td>\n",
       "      <td>0.661342</td>\n",
       "      <td>0.370273</td>\n",
       "      <td>0.969094</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.863068</td>\n",
       "      <td>0.889598</td>\n",
       "      <td>0.136221</td>\n",
       "      <td>0.633699</td>\n",
       "      <td>0.481547</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.165672</td>\n",
       "      <td>0.228258</td>\n",
       "      <td>0.511861</td>\n",
       "      <td>0.508977</td>\n",
       "      <td>0.661427</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.241224</td>\n",
       "      <td>0.374647</td>\n",
       "      <td>0.995093</td>\n",
       "      <td>0.503921</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.266167</td>\n",
       "      <td>0.116179</td>\n",
       "      <td>0.333483</td>\n",
       "      <td>0.819868</td>\n",
       "      <td>0.243572</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.189926</td>\n",
       "      <td>0.908398</td>\n",
       "      <td>0.282621</td>\n",
       "      <td>0.020797</td>\n",
       "      <td>0.391406</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.254189</td>\n",
       "      <td>0.490561</td>\n",
       "      <td>0.565905</td>\n",
       "      <td>0.342328</td>\n",
       "      <td>0.528762</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.729248</td>\n",
       "      <td>0.093026</td>\n",
       "      <td>0.286540</td>\n",
       "      <td>0.142304</td>\n",
       "      <td>0.352719</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.723943</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.277021</td>\n",
       "      <td>0.764116</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.228713</td>\n",
       "      <td>0.728065</td>\n",
       "      <td>0.636643</td>\n",
       "      <td>0.852912</td>\n",
       "      <td>0.015468</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.836372</td>\n",
       "      <td>0.714160</td>\n",
       "      <td>0.788059</td>\n",
       "      <td>0.727042</td>\n",
       "      <td>0.107409</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.170761</td>\n",
       "      <td>0.268138</td>\n",
       "      <td>0.544100</td>\n",
       "      <td>0.029530</td>\n",
       "      <td>0.058133</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.829390</td>\n",
       "      <td>0.639783</td>\n",
       "      <td>0.766233</td>\n",
       "      <td>0.577099</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.938386</td>\n",
       "      <td>0.940604</td>\n",
       "      <td>0.667872</td>\n",
       "      <td>0.196641</td>\n",
       "      <td>0.836421</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.375714</td>\n",
       "      <td>0.373530</td>\n",
       "      <td>0.686516</td>\n",
       "      <td>0.570779</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.165958</td>\n",
       "      <td>0.913484</td>\n",
       "      <td>0.945770</td>\n",
       "      <td>0.204813</td>\n",
       "      <td>0.356018</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.557046</td>\n",
       "      <td>0.462250</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.368805</td>\n",
       "      <td>0.914561</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.157139</td>\n",
       "      <td>0.822056</td>\n",
       "      <td>0.407047</td>\n",
       "      <td>0.875072</td>\n",
       "      <td>0.512658</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.484495</td>\n",
       "      <td>0.432628</td>\n",
       "      <td>0.504551</td>\n",
       "      <td>0.292628</td>\n",
       "      <td>0.825284</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.258699</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>0.227687</td>\n",
       "      <td>0.465568</td>\n",
       "      <td>0.889506</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.379843</td>\n",
       "      <td>0.524849</td>\n",
       "      <td>0.558902</td>\n",
       "      <td>0.109573</td>\n",
       "      <td>0.499802</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4  customer_id  label\n",
       "0   0.922103  0.057794  0.526038  0.148553  0.568629            1      1\n",
       "1   0.327477  0.820193  0.959226  0.419158  0.289422            2      1\n",
       "2   0.015102  0.064755  0.877044  0.400378  0.466417            3      1\n",
       "3   0.368077  0.896085  0.572603  0.500439  0.736777            4      1\n",
       "4   0.584785  0.882418  0.336971  0.228047  0.982025            5      1\n",
       "5   0.351409  0.713414  0.452940  0.687918  0.415270            6      1\n",
       "6   0.221729  0.022390  0.704187  0.787040  0.811190            7      1\n",
       "7   0.017000  0.134116  0.788072  0.501306  0.686186            8      1\n",
       "8   0.503838  0.373508  0.741935  0.597098  0.458992            9      1\n",
       "9   0.932887  0.331860  0.702883  0.884695  0.383202           10      1\n",
       "10  0.462645  0.750930  0.197640  0.822586  0.200114           11      1\n",
       "11  0.137557  0.050172  0.658260  0.209058  0.843895           12      1\n",
       "12  0.687471  0.395025  0.495986  0.647649  0.432749           13      1\n",
       "13  0.695643  0.408796  0.861199  0.377008  0.634284           14      1\n",
       "14  0.937113  0.076653  0.545357  0.023949  0.804219           15      1\n",
       "15  0.068962  0.519983  0.623262  0.940664  0.480909           16      1\n",
       "16  0.694689  0.415122  0.365560  0.467489  0.833134           17      1\n",
       "17  0.657428  0.132193  0.268241  0.296970  0.187559           18      1\n",
       "18  0.381308  0.174895  0.600854  0.929272  0.901171           19      1\n",
       "19  0.771835  0.304107  0.661342  0.370273  0.969094           20      1\n",
       "20  0.863068  0.889598  0.136221  0.633699  0.481547           21      1\n",
       "21  0.165672  0.228258  0.511861  0.508977  0.661427           22      1\n",
       "22  0.001943  0.241224  0.374647  0.995093  0.503921           23      1\n",
       "23  0.266167  0.116179  0.333483  0.819868  0.243572           24      1\n",
       "24  0.189926  0.908398  0.282621  0.020797  0.391406           25      1\n",
       "25  0.254189  0.490561  0.565905  0.342328  0.528762           26      0\n",
       "26  0.729248  0.093026  0.286540  0.142304  0.352719           27      0\n",
       "27  0.548623  0.723943  0.515140  0.277021  0.764116           28      0\n",
       "28  0.228713  0.728065  0.636643  0.852912  0.015468           29      0\n",
       "29  0.836372  0.714160  0.788059  0.727042  0.107409           30      0\n",
       "30  0.170761  0.268138  0.544100  0.029530  0.058133           31      0\n",
       "31  0.272500  0.829390  0.639783  0.766233  0.577099           32      0\n",
       "32  0.938386  0.940604  0.667872  0.196641  0.836421           33      0\n",
       "33  0.375714  0.373530  0.686516  0.570779  0.417592           34      0\n",
       "34  0.165958  0.913484  0.945770  0.204813  0.356018           35      0\n",
       "35  0.557046  0.462250  0.018061  0.368805  0.914561           36      0\n",
       "36  0.157139  0.822056  0.407047  0.875072  0.512658           37      0\n",
       "37  0.484495  0.432628  0.504551  0.292628  0.825284           38      0\n",
       "38  0.258699  0.856242  0.227687  0.465568  0.889506           39      0\n",
       "39  0.379843  0.524849  0.558902  0.109573  0.499802           40      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/4c/7557e1c2e791bd43878f8c82065bddc5798252084f26ef44527c02262af1/imbalanced_learn-0.4.3-py3-none-any.whl (166kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\kshit\\anaconda4\\lib\\site-packages (from imbalanced-learn) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\kshit\\anaconda4\\lib\\site-packages (from imbalanced-learn) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\kshit\\anaconda4\\lib\\site-packages (from imbalanced-learn) (1.1.0)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import fetch_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= fetch_datasets()['wine_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    4715\n",
       " 1     183\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    96.263781\n",
       " 1     3.736219\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %of -1 & 1\n",
    "\n",
    "pd.Series(y).value_counts()/X.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.97      0.97      1564\n",
      "           1       0.27      0.32      0.30        53\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1617\n",
      "   macro avg       0.63      0.65      0.63      1617\n",
      "weighted avg       0.95      0.95      0.95      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros= RandomOverSampler(random_state =0)\n",
    "\n",
    "X= data.data\n",
    "\n",
    "y= data.target\n",
    "\n",
    "X_resampled, y_resampled =ros.fit_resample(X,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled , test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x218b2230630>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y_resampled).value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.96      0.98      1556\n",
      "           1       0.96      1.00      0.98      1556\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      3112\n",
      "   macro avg       0.98      0.98      0.98      3112\n",
      "weighted avg       0.98      0.98      0.98      3112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "X= data.data\n",
    "\n",
    "y= data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ros= RandomOverSampler(random_state =0)\n",
    "\n",
    "X_resampled, y_resampled =ros.fit_resample(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.98      0.98      1564\n",
      "           1       0.31      0.30      0.30        53\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1617\n",
      "   macro avg       0.64      0.64      0.64      1617\n",
      "weighted avg       0.95      0.95      0.95      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X= data.data\n",
    "\n",
    "y= data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "X_resampled, y_resampled =SMOTE(k_neighbors=7).fit_resample(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.93      0.96      1564\n",
      "           1       0.22      0.55      0.31        53\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1617\n",
      "   macro avg       0.60      0.74      0.63      1617\n",
      "weighted avg       0.96      0.92      0.94      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation - wrong way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X= data.data\n",
    "\n",
    "y= data.target\n",
    "\n",
    "X_resampled, y_resampled =SMOTE(k_neighbors=7).fit_resample(X_train,y_train)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "scores= cross_val_score(model, X_resampled, y_resampled, cv=5, scoring ='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88205128, 0.88060644, 0.8846399 , 0.91457932, 0.94302554])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation - right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "X= data.data\n",
    "y = data.target\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "scores=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index] for panda dataframe , below is numpy\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = SMOTE().fit_resample(X_train,y_train)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_resampled,y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "scores.append(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12213740458015268]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    953\n",
      " 1     26\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling without data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.86321244e+00, 2.79100884e-01, 3.33697044e-01, 6.43267297e+00,\n",
       "       4.55236208e-02, 3.52875648e+01, 1.38148888e+02, 9.94053130e-01,\n",
       "       3.18971960e+00, 4.89457482e-01, 1.05131616e+01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X= data.data\n",
    "\n",
    "y= data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train).mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.97      0.97      1564\n",
      "           1       0.27      0.32      0.29        53\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1617\n",
      "   macro avg       0.62      0.65      0.63      1617\n",
      "weighted avg       0.95      0.95      0.95      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "steps = [\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "pipe= Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      1.00      0.98      1564\n",
      "           1       0.43      0.06      0.10        53\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1617\n",
      "   macro avg       0.70      0.53      0.54      1617\n",
      "weighted avg       0.95      0.97      0.95      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.33, random_state=42)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.09756098, 0.04761905, 0.13636364, 0.        , 0.05405405])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipe, X, y, cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Grid Search with pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\kshit\\Anaconda4\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.1, 'lr__penalty': 'l1'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "steps = [\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "parameters = {\n",
    "    'lr__penalty' : ['l1','l2'],\n",
    "    'lr__C' : [0.1, 0.001, 1]\n",
    "}\n",
    "\n",
    "clf= GridSearchCV(pipe, parameters, cv=5)\n",
    "clf.fit(X,y)\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
